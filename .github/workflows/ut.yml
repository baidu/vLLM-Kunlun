name: Kunlun Unit Test

on: [push, pull_request]

jobs:
  test-kunlun:
    runs-on: [self-hosted, test-0]

    env:
      http_proxy: http://10.63.229.53:8891
      https_proxy: https://10.63.229.53:8891
      no_proxy: localhost,127.0.0.1,0.0.0.0,.baidu.com,.baidu-int.com

    steps:
      # --- 第一步：环境配置 ---
      - name: Configure Environment
        run: |
          CONDA_BIN="/root/miniconda/envs/python310_torch25_cuda/bin"
          echo "$CONDA_BIN" >> $GITHUB_PATH
          echo "PY=$CONDA_BIN/python" >> $GITHUB_ENV
      
      # --- 第二步：拉取 vLLM-Kunlun 代码 ---
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          # 加上 fetch-depth: 0 避免浅克隆带来的 git 报错
          fetch-depth: 0
      
      - name: Run Unit Test
        run: |
          # 5. 运行测试
          echo "Running full suite..."
          export XPU_VISIBLE_DEVICES=1
          pytest -v -s --cov tests/ut -p no:warnings tests/ut


