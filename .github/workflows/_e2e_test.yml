name: 'e2e-test'

on:
  workflow_call:
  pull_request:            
    branches:
      - main
    types: [opened, synchronize, reopened]
  push:                    
    branches:
      - main
concurrency:
  group: e2e-singlecard
  cancel-in-progress: false
jobs:
  e2e:
    name: e2e-test-singlecard
    runs-on: 
      - self-hosted
      - Linux
      - X64
    steps:
      - name: Make /workspace writable to avoid cleanup failure
        run: |
          sudo chmod -R 777 /workspace
      - name: Checkout PR code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Verify PR workspace
        run: |
          echo "===== 1WORKSPACE ====="
          pwd
          ls -l

          echo "===== GIT INFO ====="
          git rev-parse HEAD
          git log -1 --oneline
          git status --porcelain
      - name: create start_docker
        run: |
          cd /workspace
          cat << 'EOF' > start_docker.sh
          ME_PREFIX="aiak"

          IMAGE="iregistry.baidu-int.com/xmlir/xmlir_ubuntu_2004_x86_64:v0.32"

          
          CONFIG=".config.release"
          
          WORK_DIR=$(cd $(dirname $0) && pwd) && cd ${WORK_DIR}
          
          function create_docker() {
              local IMAGE=$(get_imagename)
              local NAME=$(get_dockername)
          
              # Step 1: Detect host CUDA library path
              local HOST_CUDA_LIB_PATH=""
              for path in "/usr/local/cuda/lib64" "/usr/local/cuda-*/lib64"; do
                  if [ -d "$(echo $path)" ]; then
                      HOST_CUDA_LIB_PATH=$(echo $path)
                      break
                  fi
              done
              
              if [ -z "$HOST_CUDA_LIB_PATH" ]; then
                  echo "⚠️ Host CUDA library path not found, will use container built-in CUDA"
              else
                  echo "✓ Detected host CUDA library path: $HOST_CUDA_LIB_PATH"
              fi
          
              # Step 2: Precisely match NVIDIA devices (avoid special files like nvidia-caps)
              DEVICE_ARGS=""
              if [ -e "/dev/nvidia0" ]; then
                 # Step 2.1: Add NVIDIA control device # 步骤2.1: 获取NVIDIA控制设备
                  DEVICE_ARGS="--device /dev/nvidia0:/dev/nvidia0"
                  
                  # Step 2.2: Add other GPU devices (only match numeric suffix devices)
                  for i in $(seq 1 16); do
                      if [ -e "/dev/nvidia$i" ]; then
                          DEVICE_ARGS="$DEVICE_ARGS --device /dev/nvidia$i:/dev/nvidia$i"
                      fi
                  done
                  
                  # Step 2.3: Add required nvidia-uvm device
                  if [ -e "/dev/nvidia-uvm" ]; then
                      DEVICE_ARGS="$DEVICE_ARGS --device /dev/nvidia-uvm:/dev/nvidia-uvm"
                  fi
                  
                  # Step 2.4: Add required nvidia-modeset device
                  if [ -e "/dev/nvidia-modeset" ]; then
                      DEVICE_ARGS="$DEVICE_ARGS --device /dev/nvidia-modeset:/dev/nvidia-modeset"
                  fi
              fi
          
              # New: Mount nvidia-smi binary
              NVIDIA_BIN=""
              if [ -f "/usr/bin/nvidia-smi" ]; then
                  NVIDIA_BIN="-v /usr/bin/nvidia-smi:/usr/bin/nvidia-smi"
                  echo "✓ Added nvidia-smi mount"
              else
                  echo "⚠️ nvidia-smi not found, please check NVIDIA driver installation"
              fi
          
              # New: Mount required NVIDIA library files
              NVIDIA_LIBS=""
              if [ -d "/usr/lib64" ]; then
                  echo "✓ Mounting critical NVIDIA library files"
                  for lib in libnvidia-ml.so libnvidia-ml.so.1; do
                      if [ -f "/usr/lib64/$lib" ]; then
                          NVIDIA_LIBS="$NVIDIA_LIBS -v /usr/lib64/$lib:/usr/lib64/$lib"
                      fi
                  done
              fi
              # Step 1: Ensure correct symbolic links on host before starting Docker
              sudo ln -sf /usr/lib64/libcuda.so.1 /usr/lib64/libcuda.so
                          
              # Step 3: Create container - P800 optimized version
              echo "Starting container with device mapping parameters: $DEVICE_ARGS"
              docker run \
                  -h `hostname` \
                  --privileged \
                  --net=host \
                  --user=root \
                  --name=$NAME \
                  -v /home:/home \
                  -v /home/users/v_huchenchao/workspace:/workspace \
                  -v /ssd2:/ssd2 \
                  -v /ssd1:/ssd1 \
                  -v /ssd3:/ssd3 \
                  -v /dev/shm:/dev/shm \
                  -v /usr/lib64/libcuda.so.1:/usr/lib64/libcuda.so.1 \
                  -v /usr/lib64/libcuda.so:/usr/lib64/libcuda.so \
                  -v /usr/lib64/libnvidia-ml.so.1:/usr/lib64/libnvidia-ml.so.1 \
                  -v /usr/lib64/libnvidia-ptxjitcompiler.so.1:/usr/lib64/libnvidia-ptxjitcompiler.so.1 2>/dev/null \
                  -w /workspace \
                  $DEVICE_ARGS \
                  $NVIDIA_BIN \
                  $NVIDIA_LIBS \
                  --shm-size=16G \
                  -e NVIDIA_VISIBLE_DEVICES=all \
                  -e NVIDIA_DRIVER_CAPABILITIES=compute,utility \
                  -itd $IMAGE
          
              # Step 4: Configure environment inside the container
              sleep 2
              if [ "$(docker ps -q -f name=$NAME)" ]; then
                  echo "✓ Container created successfully! Configuring environment..."
                  # Inject CUDA library path configuration script
                  # docker exec $NAME bash -c 'ln -sf /usr/lib64/libcuda.so.1 /usr/lib64/libcuda.so'
                  # docker exec $NAME bash -c 'echo "export LD_LIBRARY_PATH=/usr/lib64:/usr/local/cuda/lib64:\$LD_LIBRARY_PATH" >> ~/.bashrc'
                  # docker exec $NAME bash -c 'echo "export VLLM_USE_CUDA_GRAPH=1" >> ~/.bashrc'
                  # docker exec $NAME bash -c 'echo "export VLLM_ATTENTION_BACKEND=FLASH_ATTENTION" >> ~/.bashrc'
                  # docker exec $NAME bash -c 'echo "export CUDA_DEVICE_MAX_CONNECTIONS=1" >> ~/.bashrc'
                  echo "✓ Environment configuration completed!"
              fi
          }
          
          function login_docker() {
              local NAME=$(get_dockername)
              docker exec -it --privileged --user root ${NAME} /bin/bash
          }
          
          function remove_docker() {
              local NAME=$(get_dockername)
          
              # This operation is dangous, should be double confirm.
              echo "This opreation will remove ${NAME} docker"
              read -p "Please Confirm (YES): " confirm
              if [ "$confirm" == "YES" ];then
                  read -p "Please Confirm Again (YES): " confirm
                  if [ "$confirm" == "YES" ];then
                      docker stop ${NAME}
                  docker rm ${NAME}
                  fi
              fi
          }
          
          function echo_usage() {
              NAME=$(get_dockername)
          
              echo -e "# DOCKER_NAME=$NAME"
              echo -e "# You can rename it by modify ${CONFIG}"
              echo -e "Usage:"
              echo -e "$0 [login]  # login docker"
              echo -e "$0 [create] # create docker"
              echo -e "$0 [remove] # remove docker"
          }
          
          function config() {
              if [ ! -f ${CONFIG} ] ;then
                  read -p "Please input your name: " name
                  DOCKER_NAME=${ME_PREFIX}-${name}
                  echo "DOCKER_NAME=${DOCKER_NAME}, you can rename it by modify ${CONFIG}"
          
                  # initial .config file
                  echo "DOCKER_NAME=${DOCKER_NAME}" > ${CONFIG}
                  echo "IMAGE_NAME=${IMAGE}" >> ${CONFIG}
              fi
          
          }
          
          function get_value() {
              local KEY=$1
          
              if [ ! -f ${CONFIG} ];then
                  config
              fi
          
              name=$(cat ${CONFIG} | grep $KEY | awk -F'=' '{print $2}' | sed 's/ //g')
          
              if [ ! -n "$name" ];then
                  config
                  name=$(cat ${CONFIG} | grep $KEY | awk -F'=' '{print $2}' | sed 's/ //g')
              fi
          
              echo ${name}
          }
          
          function get_dockername() {
              get_value "DOCKER_NAME"
          }
          
          function get_imagename() {
              get_value "IMAGE_NAME"
          }
          
          function main() {
              if [ ! -n $1 ];then
                  echo_usage
                  exit -1
              fi
          
              config
          
              if [ "$1" == 'create' ];then
                  create_docker
              elif [ "$1" == 'login' ];then
                  login_docker
              elif [ "$1" == 'remove' ];then
                  remove_docker
              else
                  echo_usage
              fi
          }
          
          main $@
          EOF
          chmod +x start_docker.sh
 

 
      - name: set config
        run: |
          cd /workspace
          cat > .config.release << 'EOF'
          DOCKER_NAME=aiak-e2e-singlecard
          IMAGE_NAME=iregistry.baidu-int.com/xmlir/xmlir_ubuntu_2004_x86_64:v0.32
          EOF

      - name: create docker
        run: |
          sudo bash /workspace/start_docker.sh create
          cat /workspace/start_docker.sh
          sudo docker exec aiak-e2e-singlecard bash -lc "
            echo 'conda activate python310_torch25_cuda' >> ~/.bashrc
          " 

      
      - name : Install vLLM 0.11.0
        run: |
          sudo docker exec aiak-e2e-singlecard bash -lc "
            #source /root/miniconda/etc/profile.d/conda.sh
            conda activate python310_torch25_cuda
            conda env list
            pip uninstall -y vllm
            env | grep -i proxy
            pip install vllm==0.11.0 --no-build-isolation --no-deps --index-url https://pip.baidu-int.com/simple/
          "
      - name : Install vLLM-kunlun 
        run: |
          sudo docker exec aiak-e2e-singlecard bash -lc "
            cd /workspace
            export {http,https}_proxy=http://agent.baidu.com:8891
            rm -rf vLLM-Kunlun
            #git clone https://github.com/baidu/vLLM-Kunlun
            #unset http_proxy&&unset https_proxy
            #cd vLLM-Kunlun
            git config --global --add safe.directory \"$GITHUB_WORKSPACE\"

            cd \"$GITHUB_WORKSPACE\"

            echo '===== USING PR CODE ====='
            git rev-parse HEAD
            git log -1 --oneline
            unset http_proxy&&unset https_proxy
            cd vLLM-Kunlun
            pip install -r requirements.txt
            python setup.py build
            python setup.py install
            cp vllm_kunlun/patches/eval_frame.py /root/miniconda/envs/python310_torch25_cuda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py
            wget -O xpytorch-cp310-torch251-ubuntu2004-x64.run https://baidu-kunlun-public.su.bcebos.com/v1/baidu-kunlun-share/1130/xpytorch-cp310-torch251-ubuntu2004-x64.run?authorization=bce-auth-v1%2FALTAKypXxBzU7gg4Mk4K4c6OYR%2F2025-12-02T05%3A01%3A27Z%2F-1%2Fhost%2Ff3cf499234f82303891aed2bcb0628918e379a21e841a3fac6bd94afef491ff7
            bash xpytorch-cp310-torch251-ubuntu2004-x64.run
            pip install "https://baidu-kunlun-public.su.bcebos.com/v1/baidu-kunlun-share/1130/xtorch_ops-0.1.2209%2B6752ad20-cp310-cp310-linux_x86_64.whl?authorization=bce-auth-v1%2FALTAKypXxBzU7gg4Mk4K4c6OYR%2F2025-12-05T06%3A18%3A00Z%2F-1%2Fhost%2F14936c2b7e7c557c1400e4c467c79f7a9217374a7aa4a046711ac4d948f460cd"
            pip install "https://cce-ai-models.bj.bcebos.com/v1/vllm-kunlun-0.11.0/triton-3.0.0%2Bb2cde523-cp310-cp310-linux_x86_64.whl"
            pip install "https://cce-ai-models.bj.bcebos.com/XSpeedGate-whl/release_merge/20251219_152418/xspeedgate_ops-0.0.0-cp310-cp310-linux_x86_64.whl"




            chmod +x "$GITHUB_WORKSPACE/vLLM-Kunlun/setup_env.sh" && source "$GITHUB_WORKSPACE/vLLM-Kunlun/setup_env.sh"
          "

      - name: Install evalscope && evalscope[perf]
        run: |
            sudo docker exec  aiak-e2e-singlecard bash -lc '
              export {http,https}_proxy=http://agent.baidu.com:8891
              export NO_PROXY=localhost,127.0.0.1,::1 && export no_proxy=localhost,127.0.0.1,::1
              pip install evalscope
              pip install 'evalscope[perf]'
            '
      - name: start server
        run: |
          sudo docker exec -d aiak-e2e-singlecard bash -lc '
          #chmod +x /workspace/vLLM-Kunlun/setup_env.sh && source /workspace/vLLM-Kunlun/setup_env.sh
          chmod +x "$GITHUB_WORKSPACE/vLLM-Kunlun/setup_env.sh" && source "$GITHUB_WORKSPACE/vLLM-Kunlun/setup_env.sh"
          rm -f /workspace/vllm.log
          export XPU_VISIBLE_DEVICES=5
          python -u -m vllm.entrypoints.openai.api_server \
            --host 0.0.0.0 \
            --port 8356 \
            --model /ssd3/models/Qwen3-VL-30B-A3B-Instruct \
            --gpu-memory-utilization 0.9 \
            --trust-remote-code \
            --max-model-len 32768 \
            --tensor-parallel-size 1 \
            --dtype float16 \
            --max_num_seqs 128 \
            --max_num_batched_tokens 32768 \
            --block-size 128 \
            --no-enable-prefix-caching \
            --no-enable-chunked-prefill \
            --distributed-executor-backend mp \
            --served-model-name Qwen3-VL-30B-A3B-Instruct \
            --compilation-config '\''{"splitting_ops": ["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_attention_with_output_kunlun","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"]}'\'' \
            2>&1 | tee /workspace/vllm.log 
          '

      - name: wait for vllm
        run: |
          sudo docker exec aiak-e2e-singlecard bash -lc '
            echo "Waiting for vLLM..."
              for i in {1..90}; do
                if curl -sf http://127.0.0.1:8356/v1/models >/dev/null; then
                  echo "vLLM is ready"
                  tail -n 500 /workspace/vllm.log || true
                  break
                fi
                sleep 5
              done
              if ! curl -sf http://127.0.0.1:8356/v1/models >/dev/null; then
                echo "vLLM start failed"
                echo "==== last 500 lines of vllm.log ===="
                tail -n 500 /workspace/vllm.log || true
                exit 1
              fi
          '
      - name: Accuracy testing
        run: |
          sudo docker exec aiak-e2e-singlecard bash -lc '
            rm -rf /workspace/evalscope_accuracy_report.log
            export {http,https}_proxy=http://agent.baidu.com:8891
            export NO_PROXY=localhost,127.0.0.1,::1 && export no_proxy=localhost,127.0.0.1,::1
            evalscope eval \
              --model Qwen3-VL-30B-A3B-Instruct \
              --api-url http://localhost:8356/v1 \
              --datasets gsm8k arc \
              --limit 10 2>&1 | tee  /workspace/evalscope_accuracy_report.log
          '
      

      - name: Performerance testing
        run: |
          sudo docker exec aiak-e2e-singlecard bash -lc '
            rm -rf /workspace/evalscope_performance_report.log
            export {http,https}_proxy=http://agent.baidu.com:8891
            export NO_PROXY=localhost,127.0.0.1,::1 && export no_proxy=localhost,127.0.0.1,::1
            evalscope perf \
              --model Qwen3-VL-30B-A3B-Instruct \
              --url "http://localhost:8356/v1/chat/completions" \
              --parallel 5 \
              --number 20 \
              --api openai \
              --dataset openqa \
              --stream  2>&1 | tee /workspace/evalscope_performance_report.log
          '
      - name: Cleanup docker
        if: always()
        run: |
          sudo docker stop aiak-e2e-singlecard || true
          sudo docker rm aiak-e2e-singlecard || true